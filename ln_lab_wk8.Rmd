---
title: "Lab Week 8: Clustering (k-means & hierarchical)"
author: "Larissa Neilson"
date: "2/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
library(janitor)
library(palmerpenguins)

# Packages for cluster analysis:
library(NbClust)
library(cluster)
library(factoextra)
library(dendextend)
library(ggdendro)
```

## Intro to cluster analysis (k-means, hierarchical)

### Part 1. K-means clustering:

#### Exploratory visualization

Is there an opportunity to cluster by species?

```{r}
ggplot(penguins) +
  geom_point(aes(x = bill_length_mm,
                 y = bill_depth_mm,
                 color = species,
                 shape = sex),
             size = 3,
             alpha = 0.7) +
  scale_color_manual(values = c("orange", "cyan", "darkmagenta"))
```

```{r}
ggplot(penguins) +
  geom_point(aes(x = flipper_length_mm,
                 y = body_mass_g,
                 color = species,
                 shape = sex),
             size = 3,
             alpha = 0.7) +
  scale_color_manual(values = c("orange", "cyan4", "darkmagenta"))
```

#### Pick the number of clusters

Note that we’re only using the four structural size measurement variables from `penguins`, which are columns 3 through 6 (hence the `penguins[3:6]` here). We also specify the minimum and maximum number of clusters we want `NbClust` to consider:

```{r}
# How many clusters do you THINK there should be?
number_est <- NbClust(penguins[3:6], min.nc = 2, max.nc = 10, method = "kmeans")
```

```{r}
# Check out the results (just look at the first summary report):
number_est
```

By these estimators, 2 is identified as the best number of clusters by the largest number of algorithms (8 / 30)...but should that change our mind? Maybe...but here I think it makes sense to still stick with 3 (a cluster for each species) and see how it does. 

#### Create a complete, scaled version of the data

We’re still going to use 3 clusters and see how it does, though there may be a case here for 2 given that Adelie & chinstrap penguins are pretty similar. We are going to do this with complete cases - in other words, for the variables we’re using to perform k-means clustering on penguins (bill length, bill depth, flipper length, body mass), we are dropping any observation (row) where any of those are missing. Keep in mind that this may not be the best option for every scenario - in other cases (e.g. when we have a large proportion of missingness), we may want to impute missing values instead.

```{r}
# Drop rows where any of the four size measurements are missing
penguins_complete <- penguins %>% 
  drop_na(bill_length_mm, bill_depth_mm, body_mass_g, flipper_length_mm)

# Only keep the columns for the four size measurements, then SCALE them
penguins_scale <- penguins_complete %>% 
  select(ends_with("mm"), body_mass_g) %>% 
  scale()
```

#### Run k-means

Now that we have complete, scaled data for the four size variables of interest, let’s run k-means. You should know the iterative process it’s running through from the Week 8 lecture.

```{r}
penguins_km <- kmeans(penguins_scale, 3) # kmeans specifying 3 groups to start
```

```{r}
# See what it returns (different elements returned by kmeans function):
penguins_km$size # How many observations assigned to each cluster
```

```{r}
penguins_km$cluster # What cluster each observation in penguins_scale is assigned to
```

```{r}
# Bind the cluster number to the original data used for clustering, so that we can see what cluster each penguin is assigned to
penguins_cl <- data.frame(penguins_complete, cluster_no = factor(penguins_km$cluster))

# Plot flipper length versus body mass, indicating which cluster each penguin is assigned to (but also showing the actual species):
ggplot(penguins_cl) +
  geom_point(aes(x = flipper_length_mm,
                 y = body_mass_g,
                 color = cluster_no,
                 shape = species))
```

```{r}
# Plot bill dimensions & map species & cluster number to the point shape and color aesthetics:
ggplot(penguins_cl) +
  geom_point(aes(x = bill_length_mm,
                 y = bill_depth_mm,
                 color = cluster_no,
                 shape = species))
```

Find counts

```{r}
# Find the counts of each species assigned to each cluster, then pivot_wider() to make it a contingency table:
penguins_cl %>% 
  count(species, cluster_no) %>% 
  pivot_wider(names_from = cluster_no, values_from = n) %>% 
  rename("Cluster 1" = "1", "Cluster 2" = "2", "Cluster 3" = "3")
```

Takeaway: as we see from the graph, most chinstraps in Cluster 3, and most Adelies in Cluster 2, and all Gentoos are in Cluster 1 by k-means clustering. So this actually does a somewhat decent job of splitting up the three species into different clusters, with some overlap in Cluster 3 between Adelies & chinstraps, which is consistent with what we observed in exploratory data visualization.

### Part 2. Cluster analysis: hierarchical

We will use the `stats::hclust()` function for agglomerative hierarchical clustering, using WorldBank environmental data (simplified), wb_env.csv.

#### Read in the data & simplify

```{r}
# Get the data
wb_env <- read_csv("wb_env.csv")

# Only keep the top 20 GHG emitters (for simplifying viz)
wb_ghg_20 <- wb_env %>% 
  arrange(-ghg) %>% 
  head(20)
```

#### Scale the data

```{r}
# Scale the numeric variables (columns 3:7)
wb_scaled <- wb_ghg_20 %>% 
  select(3:7) %>% 
  scale()

# Update to ass rownames (country name) from wb_ghg_20
rownames(wb_scaled) <- wb_ghg_20$name
```

#### Find the Euclidean distances

Use the `stats::dist()` function to find the Euclidean distance in multivariate space between the different observations (countries):

```{r}
# Compute dissimilarity values (Euclidean distances):
euc_distance <- dist(wb_scaled, method = "euclidean")
```

#### Perform hierarchical clustering by complete linkage with `stats::hclust()`

The `stats::hclust()` function performs hierarchical clustering, given a dissimilarity matrix (our matrix of euclidean distances), using a linkage that you specify.

Here, let’s use complete linkage (recall from lecture: clusters are merged by the smallest *maximum* distance between two observations in distinct clusters).

```{r}
# Hierarchical clustering (complete linkage)
hc_complete <- hclust(euc_distance, method = "complete")

# Plot it (base plot):
plot(hc_complete, cex = 0.6, hang = -1)
```

#### Now let's do it by single linkage & compare

Let’s update the linkage to single linkage (recall from lecture: this means that clusters are merged by the smallest distance between observations in separate clusters):

```{r}
# Hierarchical clustering (single linkage)
hc_single <- hclust(euc_distance, method = "single")

# Plot it (base plot):
plot(hc_single, cex = 0.6, hang = -1)
```

We see that it is a bit different when we change the linkage! But how different?

#### Make a tanglegram to compare dendograms

Let's make a **tanglegram** to compare clustering by complete & single linkage! We'll use the `dendextend::tanglegram` function to make it

First, we’ll convert to class `dendrogram`, then combine them into a list:

```{r}
# Convert to class dendrogram
dend_complete <- as.dendrogram(hc_complete)
dend_simple <- as.dendrogram(hc_single)
```

Now many a tanglegram:

```{r}
# Make a tanglegram
tanglegram(dend_complete, dend_simple)
```

That allows us to compare how things are clustered by the different linkages!

#### Plot dendrogram with ggplot instead

Here’s how you can make your dendrogram with `ggplot` (here, I’ll use the complete linkage example stored as `hc_complete`) using `ggdendrogram()`, a `ggplot` wrapper:

```{r}
ggdendrogram(hc_complete,
             rotate = TRUE) +
  theme_minimal() +
  labs(x = "Country")
```






